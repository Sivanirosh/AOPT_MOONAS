\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsthm}

\graphicspath{{figures/}}

% Define theorem styles for clarity
\newtheorem{definition}{Definition}[section]

\title{Exploring Multi-Objective Optimization in Neural Architecture Search: A Critical Review and Problem Formulation}
\author{Your Name}
\date{\today}

\begin{document}
	
	\maketitle
	
	% ------------------------------------------------------
	% SECTION: Context and Motivation
	% ------------------------------------------------------
	
	\section{Context and Motivation}
	
	Modern neural network architectures are becoming increasingly complex, posing significant challenges in balancing predictive accuracy with constraints such as latency, computational cost, and energy efficiency. Neural Architecture Search (NAS) has emerged as a pivotal AutoML sub-field to automate the discovery of neural network structures. Due to multiple conflicting objectives inherent to NAS, Multi-Objective Optimization (MOO) methods are essential to systematically explore and identify optimal trade-offs.
	
	This seminar project investigates the use and adaptation of MOO techniques within NAS frameworks, with particular focus on evolutionary and gradient-based optimization strategiesâ€”two primary approaches currently dominating NAS research, as illustrated in Fig.~\ref{fig:NAS_SSt_popularity} \cite{salmani2025systematic}. While historically Reinforcement Learning (RL) was the pioneering approach, evolutionary and gradient-based methods have increasingly attracted research interest due to their effectiveness and computational efficiency. Hence, we focus exclusively on these two strategies to provide deeper insights into their MOO foundations, adaptations, and innovations within NAS.
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.7\textwidth]{NAS_SSt_Popularity.png}
		\caption{Popularity trends of NAS search strategies (Credit: \cite{salmani2025systematic}).}
		\label{fig:NAS_SSt_popularity}
	\end{figure}
	
	% ------------------------------------------------------
	% SECTION: Background
	% ------------------------------------------------------
	
	\section{Background}
	
	To fully appreciate NAS as an MOO problem, foundational concepts from AutoML, hyperparameter optimization (HPO), NAS, and standard MOO methods are essential.
	
	\subsection{Automated Machine Learning and Related Paradigms}
	
	Automated Machine Learning (AutoML) aims at automating the end-to-end process of applying machine learning. Prominent subfields include Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS) \cite{Baratchi2024Survey}.
	
	\subsection{Hyperparameter Optimization (HPO)}
	
	HPO methods optimize hyperparameters of a fixed learning algorithm, such as learning rates, regularization terms, or batch sizes. The optimization space is typically low-to-moderate dimensional and comprises continuous or discrete variables.
	
	\subsection{Neural Architecture Search (NAS)}
	
	NAS automates the selection of the neural network structure itself, covering decisions such as layer types, layer connectivity, and the topology of connections. NAS often involves high-dimensional, combinatorial search spaces, leading to significantly higher complexity and computational costs compared to HPO.
	
	\subsection{HPO vs. NAS: A Comparative Summary}
	
	Table~\ref{tab:hpo_vs_nas} summarizes key differences between HPO and NAS.
	
	\begin{table}[h]
		\centering
		\caption{Comparison Between HPO and NAS}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{@{}lll@{}}
				\toprule
				\textbf{Aspect} & \textbf{Hyperparameter Optimization (HPO)} & \textbf{Neural Architecture Search (NAS)} \\ \midrule
				Objective & Optimize hyperparameters of fixed model & Discover optimal neural architectures \\[5pt]
				Search Space & Learning rate, batch size, regularization terms & Layer types, connections, operations, etc. \\[5pt]
				Complexity & Moderate (continuous \& categorical) & High (large, combinatorial, discrete) \\[5pt]
				Typical Techniques & Bayesian optimization, random search & RL, evolutionary methods, differentiable methods \\[5pt]
				Evaluation Cost & Lower; fixed model & High; each architecture requires training \\ \bottomrule
		\end{tabular}}
		\label{tab:hpo_vs_nas}
	\end{table}
	
	\subsection{Multi-Objective Optimization (MOO) Concepts}
	
	MOO aims to simultaneously optimize multiple conflicting objectives. Key definitions are:
	
	\begin{definition}[Pareto Dominance]
		Given two candidate solutions $x_1,x_2\in\Omega$, solution $x_2$ dominates $x_1$ if $f_m(x_2)\leq f_m(x_1)\,\forall m$, with at least one strict inequality.
	\end{definition}
	
	\begin{definition}[Pareto Optimality and Pareto Front]
		A solution $x^*$ is Pareto optimal if no other solution dominates it. The set of all Pareto-optimal solutions forms the Pareto set, and their image under objective functions forms the Pareto front.
	\end{definition}
	
	\subsection{Prominent Multi-Objective Algorithms}
	
	\begin{itemize}
		\item \textbf{Non-dominated Sorting Genetic Algorithm (NSGA-II)}: evolutionary method using Pareto-based sorting.
		\item \textbf{Non-Dominated Sorting Crisscross Algorithm (NSCA)}: efficient handling of many-objective problems.
		\item \textbf{Multiple Gradient Descent (MGD)}: gradient-based multi-objective optimization method for deep networks.
	\end{itemize}
	
	% ------------------------------------------------------
	% SECTION: Comprehensive Problem Formulation
	% ------------------------------------------------------
	
	\section{Comprehensive Review and NAS-MOO Problem Formulation}
	\label{sec:NAS_MOO_formulation}
	
	NAS inherently involves multi-objective optimization. Commonly optimized objectives include predictive performance (accuracy, perplexity) and efficiency metrics (latency, energy, complexity).
	
	Formally, NAS as MOO can be defined as:
	
	\begin{equation}
		\label{eq:nas_moo}
		\begin{aligned}
			\min_{x} \quad & F(x) = [f_1(x), f_2(x), \dots, f_M(x)]^T, \\
			\text{subject to} \quad 
			& x \in \Omega, \quad w^*(x) = \arg \min_{w}\, \mathcal{L}_{\text{train}}(x,w),
		\end{aligned}
	\end{equation}
	
	where $x$ denotes architectural variables, and $w^*(x)$ optimal model weights.
	
	\subsection{Challenges in NAS-MOO}
	
	NAS-specific challenges include large discrete spaces, noisy evaluations, expensive training, and lack of ground-truth Pareto fronts. These challenges drive innovations and adaptations in standard MOO techniques.
	
	% ------------------------------------------------------
	% SECTION: Methodology and Experiments
	% ------------------------------------------------------
	
	\section{Proposed Methodology}
	
	We will experimentally compare two key NAS search strategies:
	
	\begin{enumerate}
		\item \textbf{Gradient-based Differentiable NAS (MODNAS)}: utilizing hyper-networks and MGD to approximate Pareto fronts rapidly.
		\item \textbf{Evolutionary Multi-Objective NAS (NSGA-II)}: a robust population-based evolutionary search.
	\end{enumerate}
	
	Additionally, we will employ \textbf{Random Search (RS)} as a baseline.
	
	\subsection{Gradient-based MODNAS Framework}
	
	We adopt a cell-based NAS structure, using continuous relaxation and differentiable architecture parameters ($\alpha$). MODNAS relies on hyper-networks to predict performance across objectives, effectively applying MGD to balance conflicting gradients and discover architectures across multiple devices and objectives simultaneously.
	
	\subsection{Evolutionary NAS Framework (NSGA-II)}
	
	NSGA-II maintains a diverse population of architectures, employing Pareto-based selection, crossover, and mutation to progressively identify Pareto-optimal solutions.
	
	% ------------------------------------------------------
	% SECTION: Experiments (Future Work)
	% ------------------------------------------------------
	
	\section{Experiments and Expected Outcomes}
	
	Experiments will be performed on standard NAS benchmarks (NASBench-101, NASBench-201, DARTS, etc.), comparing MODNAS, NSGA-II, and Random Search. Key metrics include hypervolume indicator and diversity of the Pareto-front.
	
	% ------------------------------------------------------
	% SECTION: Conclusion and Perspectives
	% ------------------------------------------------------
	
	\section{Conclusion and Future Work}
	
	This project seeks deeper clarity regarding how NAS leverages and contributes to MOO. Through systematic comparison of evolutionary and gradient-based NAS, we aim to identify meaningful insights and potential generalizable contributions to broader MOO theory and methods.
	
	\bibliographystyle{plain}
	\bibliography{references}
	
\end{document}
